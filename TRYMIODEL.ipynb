{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv, GATConv\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items (movies): 131263\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dgl\n",
    "\n",
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, user_item_pairs, total_items):\n",
    "        self.user_item_pairs = user_item_pairs\n",
    "        self.total_items = total_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_item_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, pos_item = self.user_item_pairs[idx]\n",
    "        neg_item = np.random.randint(0, self.total_items)\n",
    "        while neg_item == pos_item:\n",
    "            neg_item = np.random.randint(0, self.total_items)\n",
    "        return user, pos_item, neg_item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_path = r'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\mainmodel\\graph\\hetero_graph03_with_V_T_A_features.pkl'\n",
    "\n",
    "with open(graph_path, 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "data_path = 'D:\\\\CODE\\\\multi-model knowledge graph multi-graph recommendation system\\\\data\\\\cleanuser_rating.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "total_items = df['movieId'].max() + 1  \n",
    "print(\"Total items (movies):\", total_items)\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df['userId'] = user_encoder.fit_transform(df['userId'])\n",
    "df['movieId'] = item_encoder.fit_transform(df['movieId'])\n",
    "num_users = df['userId'].nunique() \n",
    "num_items = df['movieId'].nunique()  \n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_pairs = list(zip(train_df['userId'].values, train_df['movieId'].values))\n",
    "valid_pairs = list(zip(valid_df['userId'].values, valid_df['movieId'].values))\n",
    "\n",
    "total_items = df['movieId'].max() + 1\n",
    "\n",
    "train_dataset = BPRDataset(train_pairs, total_items)\n",
    "valid_dataset = BPRDataset(valid_pairs, total_items)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHeteroGraphConv(nn.Module):\n",
    "    def __init__(self, num_users, num_items, feature_sizes, embedding_dim=128, id_embedding_size=16, num_heads=4):\n",
    "        super(EnhancedHeteroGraphConv, self).__init__()\n",
    "\n",
    "        # 用户和项目的嵌入层\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # 基于ID的节点的嵌入层\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            node_type: nn.Embedding(10000, id_embedding_size) for node_type in feature_sizes if node_type not in ['movie', 'movieimage', 'movietext', 'movieaudio']\n",
    "        })\n",
    "\n",
    "        # 特定模态特征的转换层\n",
    "        self.image_transform = nn.Linear(feature_sizes['movieimage'], 32)\n",
    "        self.text_transform = nn.Linear(feature_sizes['movietext'], 32)\n",
    "        self.audio_transform = nn.Linear(feature_sizes['movieaudio'], 32)\n",
    "\n",
    "        # 每种模态的GAT层\n",
    "        self.gat_image1 = GATConv(32, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "        self.gat_image2 = GATConv(32 * num_heads, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "\n",
    "        self.gat_text1 = GATConv(32, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "        self.gat_text2 = GATConv(32 * num_heads, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "\n",
    "        self.gat_audio1 = GATConv(32, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "        self.gat_audio2 = GATConv(32 * num_heads, 32, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "\n",
    "        # 异构图卷积层\n",
    "        self.hetero_conv = dgl.nn.HeteroGraphConv({\n",
    "            ('movie', 'has_actor', 'actor'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_actress', 'actress'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_composer', 'composer'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_director', 'director'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_editor', 'editor'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_producer', 'producer'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'has_writer', 'writer'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('movie', 'similar', 'movie'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('user', 'rates', 'movie'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True),\n",
    "            ('user', 'similar', 'user'): GATConv(embedding_dim, embedding_dim, num_heads=num_heads, allow_zero_in_degree=True)\n",
    "        })\n",
    "\n",
    "        # 用于处理拼接特征的附加层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 3 * num_heads + embedding_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, pos_item_ids, neg_item_ids, graph, features):\n",
    "        print(f\"Forward method called with parameters: {locals()}\")\n",
    "\n",
    "        user_emb = self.user_embeddings(user_ids)\n",
    "        pos_item_emb = self.item_embeddings(pos_item_ids)\n",
    "        neg_item_emb = self.item_embeddings(neg_item_ids)\n",
    "\n",
    "        if graph is not None and features is not None:\n",
    "            image_features = F.leaky_relu(self.image_transform(features['movieimage']))\n",
    "            text_features = F.leaky_relu(self.text_transform(features['movietext']))\n",
    "            audio_features = F.leaky_relu(self.audio_transform(features['movieaudio']))\n",
    "\n",
    "            # 确保特征数量与节点数量一致\n",
    "            print(f\"Image features shape: {image_features.shape}, Graph num nodes: {graph.number_of_nodes()}\")\n",
    "            image_features = self.gat_image1(graph, image_features)\n",
    "            image_features = F.leaky_relu(image_features)\n",
    "            image_features = F.dropout(image_features, p=0.2)\n",
    "            image_features = self.gat_image2(graph, image_features).flatten(1)\n",
    "\n",
    "            print(f\"Text features shape: {text_features.shape}, Graph num nodes: {graph.number_of_nodes()}\")\n",
    "            text_features = self.gat_text1(graph, text_features)\n",
    "            text_features = F.leaky_relu(text_features)\n",
    "            text_features = F.dropout(text_features, p=0.2)\n",
    "            text_features = self.gat_text2(graph, text_features).flatten(1)\n",
    "\n",
    "            print(f\"Audio features shape: {audio_features.shape}, Graph num nodes: {graph.number_of_nodes()}\")\n",
    "            audio_features = self.gat_audio1(graph, audio_features)\n",
    "            audio_features = F.leaky_relu(audio_features)\n",
    "            audio_features = F.dropout(audio_features, p=0.2)\n",
    "            audio_features = self.gat_audio2(graph, audio_features).flatten(1)\n",
    "\n",
    "            with graph.local_scope():\n",
    "                h = self.hetero_conv(graph, {\n",
    "                    'movie': torch.cat([pos_item_emb, neg_item_emb], dim=0),\n",
    "                    'user': user_emb,\n",
    "                    'actor': self.embeddings['actor'].weight,\n",
    "                    'actress': self.embeddings['actress'].weight,\n",
    "                    'composer': self.embeddings['composer'].weight,\n",
    "                    'director': self.embeddings['director'].weight,\n",
    "                    'editor': self.embeddings['editor'].weight,\n",
    "                    'producer': self.embeddings['producer'].weight,\n",
    "                    'writer': self.embeddings['writer'].weight\n",
    "                })\n",
    "\n",
    "                movie_features = h['movie']\n",
    "                combined_features = torch.cat((image_features, text_features, audio_features, movie_features), dim=1)\n",
    "                combined_features = F.leaky_relu(self.fc(combined_features))\n",
    "\n",
    "            return user_emb, pos_item_emb, combined_features\n",
    "\n",
    "        return user_emb, pos_item_emb, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bpr_loss(users, pos_items, neg_items, lambda_reg, model):\n",
    "    # 获取用户和项目的嵌入\n",
    "    user_embeddings = model.embedding_user(users)\n",
    "    pos_item_embeddings = model.embedding_item(pos_items)\n",
    "    neg_item_embeddings = model.embedding_item(neg_items)\n",
    "    \n",
    "    # 计算用户对正样本和负样本的偏好预测\n",
    "    pos_scores = torch.sum(user_embeddings * pos_item_embeddings, dim=1)\n",
    "    neg_scores = torch.sum(user_embeddings * neg_item_embeddings, dim=1)\n",
    "    \n",
    "    # 使用 logsigmoid 提高数值稳定性\n",
    "    loss = -torch.mean(F.logsigmoid(pos_scores - neg_scores))\n",
    "    \n",
    "    # 添加 L2 正则化\n",
    "    reg_loss = lambda_reg * (user_embeddings.norm(p=2).pow(2) + \n",
    "                             pos_item_embeddings.norm(p=2).pow(2) +\n",
    "                             neg_item_embeddings.norm(p=2).pow(2))\n",
    "    \n",
    "    return loss + reg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評估函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dcg_at_k(scores, k=10):\n",
    "    ranks = torch.log2(torch.arange(2, k+2).float()).to(scores.device)  # Log term in DCG formula\n",
    "    return (scores[:k] / ranks).sum()  # Only consider the top k scores\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_scores, true_relevance, k=5):\n",
    "    _, indices = torch.sort(predicted_scores, descending=True)\n",
    "    true_sorted_by_pred = true_relevance[indices]\n",
    "    ideal_sorted, _ = torch.sort(true_relevance, descending=True)\n",
    "\n",
    "    dcg = dcg_at_k(true_sorted_by_pred[:k])\n",
    "    idcg = dcg_at_k(ideal_sorted[:k])\n",
    "    return (dcg / idcg).item() if idcg > 0 else 0.0\n",
    "def recall_at_k(predicted_scores, true_labels, k):\n",
    "    _, indices = torch.sort(predicted_scores, descending=True)\n",
    "    true_sorted_by_pred = true_labels[indices]\n",
    "    return true_sorted_by_pred[:k].float().sum().item() / true_labels.float().sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_and_features(user_ids, pos_item_ids, neg_item_ids, device):\n",
    "    user_ids = user_ids.to(device)\n",
    "    pos_item_ids = pos_item_ids.to(device)\n",
    "    neg_item_ids = neg_item_ids.to(device)\n",
    "\n",
    "    # Ensure indices are unique and continuous across the whole graph\n",
    "    all_users = torch.cat([user_ids, user_ids])  # Users are duplicated for pos and neg connections\n",
    "    all_items = torch.cat([pos_item_ids, neg_item_ids])  # All item connections\n",
    "\n",
    "    # Create mappings for user and item IDs to continuous indices\n",
    "    unique_ids, inverse_indices = torch.unique(torch.cat([user_ids, all_items]), return_inverse=True)\n",
    "    user_indices = inverse_indices[:len(user_ids)]\n",
    "    item_indices = inverse_indices[len(user_ids):]\n",
    "\n",
    "    # Create the graph\n",
    "    src = torch.cat([user_indices, user_indices])  # Users for both pos and neg items\n",
    "    dst = item_indices  # Combined pos and neg items\n",
    "    g = dgl.graph((src, dst), num_nodes=unique_ids.numel()).to(device)\n",
    "\n",
    "    # Add self-loops to the graph\n",
    "    g = dgl.add_self_loop(g)\n",
    "\n",
    "    # Assign features - assuming random features for simplicity\n",
    "    g.ndata['feat'] = torch.randn(g.number_of_nodes(), 16).to(device)\n",
    "\n",
    "    # Ensure the number of item features matches the number of unique item nodes\n",
    "    num_items = g.number_of_nodes() - len(user_ids)\n",
    "    features = {\n",
    "        'movieimage': torch.randn(num_items, 2048).to(device),\n",
    "        'movietext': torch.randn(num_items, 384).to(device),\n",
    "        'movieaudio': torch.randn(num_items, 128).to(device)\n",
    "    }\n",
    "\n",
    "    print(f\"Graph has {g.number_of_nodes()} nodes\")\n",
    "    for key, value in features.items():\n",
    "        print(f\"Feature {key} has shape {value.shape}\")\n",
    "\n",
    "    return g, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs, lambda_reg, optimizer, device, validation_loader=None):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = data\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            # 创建图结构和特征\n",
    "            graph, features = create_graph_and_features(user_ids, pos_item_ids, neg_item_ids, device)\n",
    "\n",
    "            # 输出调试信息\n",
    "            print(f\"Epoch {epoch+1}:\")\n",
    "            print(f\"user_ids: {user_ids.shape}, pos_item_ids: {pos_item_ids.shape}, neg_item_ids: {neg_item_ids.shape}\")\n",
    "            print(f\"graph: {graph}\")\n",
    "            print(f\"features: {[key for key in features.keys()]}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            user_embeddings, pos_item_embeddings, combined_features = model(user_ids, pos_item_ids, neg_item_ids, graph, features)\n",
    "            \n",
    "            neg_item_embeddings = combined_features if combined_features is not None else pos_item_embeddings  # 使用 combined_features 作為負樣本嵌入\n",
    "\n",
    "            # 计算损失\n",
    "            loss = bpr_loss(user_embeddings, pos_item_embeddings, neg_item_embeddings, lambda_reg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Average Training Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        if validation_loader:\n",
    "            evaluate_metrics(model, validation_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_metrics(model, validation_loader, device, k=5):\n",
    "    device = next(model.parameters()).device  # 获取模型参数所在的设备\n",
    "    model.eval()\n",
    "    total_dcg = 0\n",
    "    total_ndcg = 0\n",
    "    total_recall = 0\n",
    "    total_relevant_items = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = data\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "            \n",
    "            # 创建图结构和特征\n",
    "            graph, features = create_graph_and_features(user_ids, pos_item_ids, neg_item_ids, device)\n",
    "            \n",
    "            # 前向传播\n",
    "            user_embeddings, pos_item_embeddings, combined_features = model(user_ids, pos_item_ids, graph, features)\n",
    "            neg_item_embeddings = combined_features  # 使用combined_features作为负样本嵌入\n",
    "            \n",
    "            # 计算评分\n",
    "            pos_scores = (user_embeddings * pos_item_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_item_embeddings).sum(dim=1)\n",
    "            \n",
    "            scores = torch.cat((pos_scores, neg_scores))\n",
    "            labels = torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores)))  # 真实标签\n",
    "            \n",
    "            # 计算分数并排序\n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            sorted_labels = labels[indices]\n",
    "            \n",
    "            # 使用已定义的函数计算 DCG 和 NDCG\n",
    "            dcg_value = dcg_at_k(sorted_labels, k)\n",
    "            idcg_value = dcg_at_k(torch.ones(k).to(device), k)  # 理想情况下的排序\n",
    "            ndcg_value = dcg_value / max(idcg_value, 1e-10)  # 避免除以零\n",
    "            recall_value = sorted_labels[:k].sum() / labels.sum()\n",
    "\n",
    "            total_dcg += dcg_value\n",
    "            total_ndcg += ndcg_value\n",
    "            total_recall += recall_value\n",
    "            total_relevant_items += labels.sum()\n",
    "\n",
    "    average_dcg = total_dcg / len(validation_loader)\n",
    "    average_ndcg = total_ndcg / len(validation_loader)\n",
    "    average_recall = total_recall / len(validation_loader)\n",
    "    print(f'Average DCG@{k}: {average_dcg:.4f}')\n",
    "    print(f'Average NDCG@{k}: {average_ndcg:.4f}')\n",
    "    print(f'Average Recall@{k}: {average_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_sizes = {\n",
    "    'movie': 2560,    # 如果电影节点存储所有多媒体特征的总和\n",
    "    'actor': 4566,      # 演员的嵌入维度\n",
    "    'actress': 3701,    # 女演员的嵌入维度\n",
    "    'director': 4010,   # 导演的嵌入维度\n",
    "    'producer': 3952,   # 制片的嵌入维度\n",
    "    'movieimage': 2048,  # 图像特征维度\n",
    "    'movietext': 384,    # 文字特征维度\n",
    "    'movieaudio': 128,   # 音频特征维度\n",
    "    'user': 12171,        # 假设用戶的嵌入维度，可能需要根据用户数量和系统复杂性进行调整\n",
    "    'composer': 512,\n",
    "    'editor':512,\n",
    "    'writer':512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 358 nodes\n",
      "Feature movieimage has shape torch.Size([230, 2048])\n",
      "Feature movietext has shape torch.Size([230, 384])\n",
      "Feature movieaudio has shape torch.Size([230, 128])\n",
      "Epoch 1:\n",
      "user_ids: torch.Size([128]), pos_item_ids: torch.Size([128]), neg_item_ids: torch.Size([128])\n",
      "graph: Graph(num_nodes=358, num_edges=614,\n",
      "      ndata_schemes={'feat': Scheme(shape=(16,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "features: ['movieimage', 'movietext', 'movieaudio']\n",
      "Forward method called with parameters: {'self': EnhancedHeteroGraphConv(\n",
      "  (user_embeddings): Embedding(12171, 128)\n",
      "  (item_embeddings): Embedding(5996, 128)\n",
      "  (embeddings): ModuleDict(\n",
      "    (actor): Embedding(10000, 16)\n",
      "    (actress): Embedding(10000, 16)\n",
      "    (director): Embedding(10000, 16)\n",
      "    (producer): Embedding(10000, 16)\n",
      "    (user): Embedding(10000, 16)\n",
      "    (composer): Embedding(10000, 16)\n",
      "    (editor): Embedding(10000, 16)\n",
      "    (writer): Embedding(10000, 16)\n",
      "  )\n",
      "  (image_transform): Linear(in_features=2048, out_features=32, bias=True)\n",
      "  (text_transform): Linear(in_features=384, out_features=32, bias=True)\n",
      "  (audio_transform): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (gat_image1): GATConv(\n",
      "    (fc): Linear(in_features=32, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat_image2): GATConv(\n",
      "    (fc): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat_text1): GATConv(\n",
      "    (fc): Linear(in_features=32, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat_text2): GATConv(\n",
      "    (fc): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat_audio1): GATConv(\n",
      "    (fc): Linear(in_features=32, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat_audio2): GATConv(\n",
      "    (fc): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (hetero_conv): HeteroGraphConv(\n",
      "    (mods): ModuleDict(\n",
      "      (('movie', 'has_actor', 'actor')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_actress', 'actress')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_composer', 'composer')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_director', 'director')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_editor', 'editor')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_producer', 'producer')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'has_writer', 'writer')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('movie', 'similar', 'movie')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('user', 'rates', 'movie')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (('user', 'similar', 'user')): GATConv(\n",
      "        (fc): Linear(in_features=128, out_features=512, bias=False)\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "), 'user_ids': tensor([ 4567,  8770, 10475,  6069,  2442,  4710,   698,  5279,  1936,  7221,\n",
      "         5736,  7452, 10341,  9676,  9048,  7227, 10721,  2853,  6559,  6159,\n",
      "         3627, 12094,  7691,  8936,  8858,  5281,  1278,  6325,  8879,  1618,\n",
      "        10601, 10626,  7100,  7775,  8298,  6893,  6805,  7327,  4601,  3193,\n",
      "         9265,   231,  1766, 11405,  2383,  3930,  9075,  7025,  1753,  6540,\n",
      "        10259, 10353,  6961,   973,  7998,  7307,   372,   737,  5215,  9644,\n",
      "        11399,  3598,  8080,  5063, 10353,  1781,  5039,  3990,  7511,  5588,\n",
      "        11928,  2567,  6803,  2476,  9620,  4435,   885,  7899, 10227, 11307,\n",
      "         5586,  8895, 10512,   900,  4605,  5960,   803, 10957, 10097, 10838,\n",
      "         7840,  1590,  1664,  5011,  5697,   638,  8078,  5903,  4692,  9277,\n",
      "         8816,  6513,  5418,  3155, 11301,  7057,  3727, 11520,  4518,  7178,\n",
      "         2334,  1105,  7683,  7824,  5623, 10961,  9183,  6009, 11856,  3685,\n",
      "         5818,  4747,   356,  6769,  4846,  4929,  5701, 11677],\n",
      "       device='cuda:0'), 'pos_item_ids': tensor([2136, 5171,  160,  649,  185, 1774, 1755, 2314, 3499,  190, 3592, 1233,\n",
      "         989,   35, 2620, 4795,  551,  152,  848,  418,  228,  862,  701, 1712,\n",
      "        1758, 3562, 2852,  228,  596,   33, 1755, 4251,  527, 2142, 1434,  351,\n",
      "        4801,  208,  898,   37,   15,   90, 3984,  228,  862,   49,   90,  179,\n",
      "        2122,  355, 2321, 2415, 1430, 2620,  208, 3435,  798, 1120, 3753,   87,\n",
      "           1,  252, 1089,  122,    1, 1266,  862,   33, 1041, 1774,  228, 4331,\n",
      "        4443, 1320, 4778,  758, 1859, 1904, 1488,  862, 3086,  256, 3507, 1152,\n",
      "         307,  758,  986, 1635,  604,  181, 3034,  901, 2796,  195, 1984,   25,\n",
      "         192,  228, 3267, 1758, 2114,  208, 2145, 4379, 3195,  758, 1750, 3721,\n",
      "         252,  337, 2340,  360, 1156, 3086, 4520, 1518, 3031, 1046, 4332,  358,\n",
      "        1067,  234,  228, 1215, 1901, 1027,  835,   84], device='cuda:0'), 'neg_item_ids': tensor([3150, 1440, 2242, 5437, 5733, 3353, 2085,   26, 3003, 1994, 1162, 5856,\n",
      "        5220,  748, 2782, 4982, 1162, 3722,   63, 5824, 2642, 1861, 4163, 4313,\n",
      "        4154, 3659,  646, 3226, 2911, 5471, 4461, 5815, 4063, 5090, 5908, 1622,\n",
      "        1911, 1182, 1159, 3344, 4410, 3122, 4400, 2253, 5149, 4077, 3385, 3476,\n",
      "         963, 4573, 3256, 3758, 2872, 2903,  242, 4808, 4679, 5047, 1952, 3244,\n",
      "        4667, 3146, 1943, 5098,  594, 3172, 5867, 1853, 5628, 2881, 1098, 5041,\n",
      "        3899,  663, 1621, 1966, 5534, 5010,  926, 5903,  508, 2195, 2503, 2593,\n",
      "        2416, 3753, 2124, 2197, 4625, 4209, 1942,  416, 3438,   67, 3621, 4962,\n",
      "        2978, 4720, 1583, 5869, 3290,  373, 1644, 1862, 1121, 5669, 2105, 4291,\n",
      "        2083, 4499, 3927,  606, 5292, 5992, 2696, 2617, 1547, 2242, 5666, 5229,\n",
      "        3853, 2879, 4775, 3312,  164,  762, 5921, 2980], device='cuda:0'), 'graph': Graph(num_nodes=358, num_edges=614,\n",
      "      ndata_schemes={'feat': Scheme(shape=(16,), dtype=torch.float32)}\n",
      "      edata_schemes={}), 'features': {'movieimage': tensor([[ 1.3123,  0.5977,  0.8973,  ..., -0.1984,  0.4622, -0.0236],\n",
      "        [-0.3924, -1.2643,  0.2561,  ..., -1.6497, -1.4326, -1.7746],\n",
      "        [-2.1996, -0.2987, -0.3441,  ..., -0.0607, -1.5687,  0.2326],\n",
      "        ...,\n",
      "        [ 1.9527,  0.0790,  1.4464,  ..., -0.2277, -1.9206,  0.0333],\n",
      "        [-0.5407,  1.1109, -1.0440,  ..., -0.3493,  0.2965,  0.2454],\n",
      "        [ 0.4032,  2.0420, -1.8083,  ...,  1.2533, -0.8375, -0.0700]],\n",
      "       device='cuda:0'), 'movietext': tensor([[ 0.0255,  0.1608,  0.0865,  ...,  1.4072,  1.1411, -0.6118],\n",
      "        [-1.1699, -0.0489, -0.3988,  ..., -0.8620,  0.7765, -0.5530],\n",
      "        [-0.8607,  1.8711, -1.5852,  ..., -1.2943, -0.4618,  0.3551],\n",
      "        ...,\n",
      "        [-0.0744,  0.0332, -0.2844,  ..., -1.3056, -0.9930,  1.4427],\n",
      "        [-0.7460,  1.7560,  1.5001,  ..., -0.2966, -0.9111, -0.1122],\n",
      "        [ 3.4285,  1.0049, -0.7175,  ...,  0.0215,  0.9308,  0.3477]],\n",
      "       device='cuda:0'), 'movieaudio': tensor([[ 0.0876, -1.0705,  1.8240,  ..., -1.5520,  1.2575, -0.8146],\n",
      "        [-0.2796, -0.8532,  2.7586,  ...,  1.4699, -1.0936, -0.2059],\n",
      "        [ 0.1517,  0.0323, -0.9089,  ..., -1.3902, -2.1880,  0.9920],\n",
      "        ...,\n",
      "        [ 0.0588,  0.6441,  0.4404,  ...,  0.8417, -0.4361,  0.3750],\n",
      "        [-0.5117,  1.9106,  0.8615,  ...,  1.9517,  0.7574, -0.1695],\n",
      "        [ 0.5786,  0.9427, -1.4477,  ...,  0.2591,  1.9964,  0.0222]],\n",
      "       device='cuda:0')}}\n",
      "Image features shape: torch.Size([230, 32]), Graph num nodes: 358\n"
     ]
    },
    {
     "ename": "DGLError",
     "evalue": "Expect number of features to match number of nodes (len(u)). Got 230 and 358 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m EnhancedHeteroGraphConv(num_users, num_items, feature_sizes)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmulti-model knowledge graph multi-graph recommendation system\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, epochs, lambda_reg, optimizer, device, validation_loader)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[key\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mkey\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m user_embeddings, pos_item_embeddings, combined_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_item_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_item_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m neg_item_embeddings \u001b[38;5;241m=\u001b[39m combined_features \u001b[38;5;28;01mif\u001b[39;00m combined_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pos_item_embeddings  \u001b[38;5;66;03m# 使用 combined_features 作為負樣本嵌入\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 65\u001b[0m, in \u001b[0;36mEnhancedHeteroGraphConv.forward\u001b[1;34m(self, user_ids, pos_item_ids, neg_item_ids, graph, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 确保特征数量与节点数量一致\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_features\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Graph num nodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mnumber_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_image1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m image_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(image_features)\n\u001b[0;32m     67\u001b[0m image_features \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(image_features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\gatconv.py:334\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, graph, feat, edge_weight, get_attention)\u001b[0m\n\u001b[0;32m    332\u001b[0m el \u001b[38;5;241m=\u001b[39m (feat_src \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_l)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    333\u001b[0m er \u001b[38;5;241m=\u001b[39m (feat_dst \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_r)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 334\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrcdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m graph\u001b[38;5;241m.\u001b[39mdstdata\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mer\u001b[39m\u001b[38;5;124m\"\u001b[39m: er})\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\_collections_abc.py:832\u001b[0m, in \u001b[0;36mMutableMapping.update\u001b[1;34m(self, other, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Mapping):\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m other:\n\u001b[1;32m--> 832\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m other[key]\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m other\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\dgl\\view.py:99\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, (\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe HeteroNodeDataView has only one node type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease pass a tensor directly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     )\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_n_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ntid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\GNN\\lib\\site-packages\\dgl\\heterograph.py:4344\u001b[0m, in \u001b[0;36mDGLGraph._set_n_repr\u001b[1;34m(self, ntid, u, data)\u001b[0m\n\u001b[0;32m   4342\u001b[0m nfeats \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mshape(val)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   4343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nfeats \u001b[38;5;241m!=\u001b[39m num_nodes:\n\u001b[1;32m-> 4344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect number of features to match number of nodes (len(u)).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nfeats, num_nodes)\n\u001b[0;32m   4347\u001b[0m     )\n\u001b[0;32m   4348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcontext(val) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   4349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4350\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assign node feature \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to a graph on\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   4351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Call DGLGraph.to() to copy the graph to the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key, F\u001b[38;5;241m.\u001b[39mcontext(val), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4353\u001b[0m     )\n",
      "\u001b[1;31mDGLError\u001b[0m: Expect number of features to match number of nodes (len(u)). Got 230 and 358 instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# 初始化和配置模型，加载数据，然后开始训练和评估\n",
    "model = EnhancedHeteroGraphConv(num_users, num_items, feature_sizes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "train_model(model, train_loader,15, 0.01, optimizer, device, valid_loader)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth')\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('D:\\CODE\\multi-model knowledge graph multi-graph recommendation system\\code\\data\\model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/11測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rankings_and_ground_truth(model, data_loader, device):\n",
    "    model.eval()\n",
    "    rank_list = {}\n",
    "    ground_truth = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            user_ids, pos_item_ids, neg_item_ids = batch\n",
    "            user_ids, pos_item_ids, neg_item_ids = user_ids.to(device), pos_item_ids.to(device), neg_item_ids.to(device)\n",
    "\n",
    "            # 调用模型\n",
    "            pos_outputs = model(user_ids, pos_item_ids)\n",
    "            neg_outputs = model(user_ids, neg_item_ids)\n",
    "\n",
    "            # 根据输出选择得分计算方式\n",
    "            if len(pos_outputs) == 3:\n",
    "                pos_scores = (pos_outputs[0] * pos_outputs[2]).sum(dim=1)\n",
    "                neg_scores = (neg_outputs[0] * neg_outputs[2]).sum(dim=1)\n",
    "            else:\n",
    "                pos_scores = (pos_outputs[0] * pos_outputs[1]).sum(dim=1)\n",
    "                neg_scores = (neg_outputs[0] * neg_outputs[1]).sum(dim=1)\n",
    "\n",
    "            scores = torch.cat((pos_scores, neg_scores), dim=0)\n",
    "            all_item_ids = torch.cat((pos_item_ids, neg_item_ids), dim=0)\n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            sorted_items = all_item_ids[indices]\n",
    "\n",
    "            for user_id, item in zip(user_ids, sorted_items):\n",
    "                user = user_id.item()\n",
    "                item = item.item() if hasattr(item, 'item') else item  # 确保item是单个元素\n",
    "                if user not in rank_list:\n",
    "                    rank_list[user] = []\n",
    "                rank_list[user].append(item)\n",
    "\n",
    "                if user not in ground_truth:\n",
    "                    ground_truth[user] = set()\n",
    "                ground_truth[user].update(pos_item_ids.tolist())\n",
    "\n",
    "    return rank_list, ground_truth\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFTORGPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
